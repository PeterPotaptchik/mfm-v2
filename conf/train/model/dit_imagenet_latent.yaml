_target_: distcfm.models.DiTMFM
# TODO: Need to add these in
learn_loss_weighting: true
# weight_on_t_cond: true

# === tokenization ===
input_size: ${dataset.img_resolution}
patch_size: 2
in_channels: ${dataset.img_channels}

# === backbone size DiT_L_2 ===
hidden_size: 1152          # D
depth: 28                 # number of transformer blocks
num_heads: 16              # heads per block

# === conditioning ===
label_dim: 1000

# === attention impl ===
# use_fused_attn: true      # uses PyTorch SDPA (FlashAttention path when available)
  
use_joint_attention: true   # use joint spatial+channel attention