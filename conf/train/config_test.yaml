defaults:
  - _self_
  - model: unet_cifar
  - dataset: cifar 
  - SI: linear

from_pretrained: false
resume_from_checkpoint: null
use_parametrization: false
compile: true

optimizer: "Adam"
seed: 42

loss:
  distillation_type: psd
  fm_loss_type: l2
  fm_adaptive_loss_p: 1.0
  fm_adaptive_loss_c: 0.01
  distillation_loss_type: lv
  distill_teacher_stop_grad: true
  distill_adaptive_loss_p: 0.5
  distill_adaptive_loss_c: 0.01
  explicit_v00_train: false
  distillation_weight: 1.0
  distill_fm_weight: 1.0
  fm_weight: 1.0
  distill_fm: false
  distill_fm_loss_type: l2
  model_guidance: false
  model_guidance_distill_base_prob: 0.5

weighting_model:
  _target_: distcfm.models.base_model.LossWeightingNetwork
  channels: 128
  clamp_min: -10.0
  clamp_max: 10.0


lr:
  val: 0.0003
  scheduler: constant 
  warmup_steps: 0 
  min_lr: 0.00001

trainer:
  num_train_steps: 100000 
  num_warmup_steps: 10000 
  num_no_posterior_steps: 0
  num_anneal_posterior_steps: 0
  t_cond_warmup_steps: 0
  t_cond_0_rate: 0.0
  t_cond_power: 1.0
  t_cond_anneal_end_step: 0
  anneal_end_step: 50000
  batch_size: 128 
  ema:
    decay: 0.999
  devices: "auto"
  log_every_n_steps: 25
  accumulate_grad_batches: 1
  precision: "bf16-mixed"
  gradient_clip_val: 0.1

sampling:
  every_n_steps: 10000
  batch_size: 4
  vae_batch_size: 64
  n_conditioning_samples: 32
  n_samples_per_image: 4
  n_unconditional_samples: 64
  conditioning_times:
  - 0.00
  - 0.20
  - 0.30
  - 0.40
  n_kernel_steps: 
  - 10
  kernel_churn: 0.0
  n_v00_steps: 250
  cfg_scales:
  - 1.0
  - 1.5
  - 1.75
  - 2.0
  - 3.0
  x_cond_scales:
  - 1.5
  - 2.0
  - 3.0

ode_sampling_cfg:
  posterior_sampler: ode
  ode:
    steps: 100

consistency_sampling_cfg:
  posterior_sampler: consistency
  consistency:
    steps: 1
  steps_to_test:
  - 1
  - 4

wandb:
  project: ablations
  entity: distcfm
  name: default

data_dir: /vols/bitbucket/saravanan/distributional-mf/data